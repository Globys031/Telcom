from sklearn.datasets import make_blobs
from sklearn.ensemble import IsolationForest
from sklearn.decomposition import PCA

from numpy import quantile, random, where
import numpy as np
import pandas as pd
import warnings

from AnomalyDetection import anomalyIllustrator, config

###########
# The "getAnomalyIndexes" and "drawAnomaliesAllColumns" display anomalies based on all columns
# The rest of the code goes by each column and further defines why each column's data
# can be considered an anomaly.
###########

def getModel():
  return IsolationForest(n_estimators=100, max_samples='auto', contamination=0.01,
                      max_features=1.0, bootstrap=False, n_jobs=-1, random_state=42, verbose=0)

# Currently there's no contamination as there's no attack data examples. But you can't have
# contamination = 0. So for now 0.01 will suffice.
# TO DO: gather (or generate yourself) attack data. We'll later use the algorithm to check if
# that data appears as anomalies
def getAnomalyIndexes(df):
  # We're going to use all columns for anomaly detection
  # The only column that might not be of importance is "time", but for now we'll leave it as is.
  # contamination - percentage of outlier points in the data
  model=IsolationForest(n_estimators=100, max_samples='auto', contamination=0.01,
                        max_features=1.0, bootstrap=False, n_jobs=-1, random_state=42, verbose=0)
  # Fits the models and makes predictions on whether there's outliers

  model.fit(df.iloc[:,1:].values)
  df['anomaly'] = model.predict(df.iloc[:,1:].values)
  outliers = df.loc[df['anomaly'] == -1]

  # Gets the indexes of data that are anomalies
  return list(outliers.index)

# Identifies anomalies in one column as specified by index 'i'
def identifyAnomaliesForColumn(model, df, i):
  # "load_date" is only used for graphical purposes so it needs to be skipped
  # uses isolation forest for each column separately. This weird way of declaring is used to preserve
  # the column name and get only the data for that column
  model.fit(df.iloc[:,i:i+1])
  pred = model.predict(df.iloc[:,i:i+1])

  test_df = pd.DataFrame()
  test_df['load_date'] = df['load_date']
  #Find decision function to find the score and classify anomalies
  test_df['score'] = model.decision_function(df.iloc[:,i:i+1])
  # 'actuals' is the actual column data generated by dataGenerator
  test_df['actuals'] = df.iloc[:,i:i+1]
  # 'anomaly' aren't anomalies. They're predictions (1 or -1).
  test_df['anomaly'] = pred

  return classifyMetricAnomalies(test_df,df.columns[i])

# Takes one column (metric_name) as an argument
# and classifies what data in that column is not an anomaly/lowanomaly/high anomaly
def classifyMetricAnomalies(df, metric_name):
  df = df.sort_values(by='load_date', ascending=False)

  # Shift actuals (data generated by dataGenerator) 
  # by one timestamp to find the percentage change between current and previous data point
  df['shift'] = df['actuals'].shift(-1)

  df['percentage_change'] = ((df['actuals'] - df['shift']) / df['actuals']) * 100

  # Anomalies are further categorised as:
  # 0-no anomaly, 1- low anomaly , 2 - high anomaly
  df['anomaly'].loc[df['anomaly'] == 1] = 0
  df['anomaly'].loc[df['anomaly'] == -1] = 2

  max_anomaly_score = df['score'].loc[df['anomaly'] == 2].max()
  medium_percentile = df['score'].quantile(0.24)
  df['anomaly'].loc[(df['score'] > max_anomaly_score) & (df['score'] <= medium_percentile)] = 1

  return df