from sklearn.datasets import make_blobs
from sklearn.ensemble import IsolationForest
from sklearn.decomposition import PCA

from numpy import quantile, random, where
import numpy as np
import pandas as pd
import warnings

from AnomalyDetection.anomalyIllustrator import AnomalyIllustrator

###########
# The "getAnomalyIndexes" and "drawAnomaliesAllColumns" display anomalies based on all columns
# The rest of the code goes by each column and further defines why each column's data
# can be considered an anomaly.
###########

class AnomalyDetector(AnomalyIllustrator):
  # Currently there's no contamination as there's no attack data examples. But you can't have
  # contamination = 0. So for now 0.01 will suffice.
  # TO DO: gather (or generate yourself) attack data. We'll later use the algorithm to check if
  # that data appears as anomalies
  def getAnomalyIndexes(self):
    # We're going to use all columns for anomaly detection
    # The only column that might not be of importance is "time", but for now we'll leave it as is.
    # contamination - percentage of outlier points in the data
    model=IsolationForest(n_estimators=100, max_samples='auto', contamination=0.01,
                          max_features=1.0, bootstrap=False, n_jobs=-1, random_state=42, verbose=0)
    # Fits the models and makes predictions on whether there's outliers

    model.fit(self.learning_data.iloc[:,1:].values)
    self.learning_data['anomaly'] = model.predict(self.learning_data.iloc[:,1:].values)
    outliers = self.learning_data.loc[self.learning_data['anomaly'] == -1]

    # Gets the indexes of data that are anomalies
    return list(outliers.index)

  #######################################################################
  #### Below is the actual meat of the code
  #######################################################################

  # Identifies anomalies in all columns(metrics)
  def identifyAnomalies(self):
    model=IsolationForest(n_estimators=100, max_samples='auto', contamination=0.01,
                          max_features=1.0, bootstrap=False, n_jobs=-1, random_state=42, verbose=0)
    warnings.filterwarnings('ignore')

    # "load_date" is only used for graphical purposes so it needs to be skipped
    for i in range(1,len(self.learning_data.columns)-1):
      # uses isolation forest for each column separately. This weird way of declaring is used to preserve
      # the column name and get only the data for that column
      model.fit(self.learning_data.iloc[:,i:i+1])
      pred = model.predict(self.learning_data.iloc[:,i:i+1])

      test_df = pd.DataFrame()
      test_df['load_date'] = self.learning_data['load_date']
      #Find decision function to find the score and classify anomalies
      test_df['score'] = model.decision_function(self.learning_data.iloc[:,i:i+1])
      # 'actuals' is the actual column data generated by dataGenerator
      test_df['actuals'] = self.learning_data.iloc[:,i:i+1]
      # 'anomaly' aren't anomalies. They're predictions (1 or -1).
      test_df['anomaly'] = pred

      test_df = self.classifyMetricAnomalies(test_df, self.learning_data.columns[i])
      self.plotAnomalies(test_df,self.learning_data.columns[i])
  

  # Takes one column (metric_name) as an argument
  # and classifies what data in that column is not an anomaly/lowanomaly/high anomaly
  def classifyMetricAnomalies(self, df, metric_name):
    df = df.sort_values(by='load_date', ascending=False)

    # Shift actuals (data generated by dataGenerator) 
    # by one timestamp to find the percentage change between current and previous data point
    df['shift'] = df['actuals'].shift(-1)

    df['percentage_change'] = ((df['actuals'] - df['shift']) / df['actuals']) * 100

    # Anomalies are further categorised as:
    # 0-no anomaly, 1- low anomaly , 2 - high anomaly
    df['anomaly'].loc[df['anomaly'] == 1] = 0
    df['anomaly'].loc[df['anomaly'] == -1] = 2

    max_anomaly_score = df['score'].loc[df['anomaly'] == 2].max()
    medium_percentile = df['score'].quantile(0.24)
    df['anomaly'].loc[(df['score'] > max_anomaly_score) & (df['score'] <= medium_percentile)] = 1

    return df